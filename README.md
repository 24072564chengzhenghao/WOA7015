# WOA7015

Radiology Visual Question Answering (Rad-VQA) aims to answer natural language questions based on radiological images by jointly understanding visual and textual information. Compared to traditional image classification tasks, Rad-VQA better reflects real clinical information needs but poses greater challenges due to its multimodal nature. Different modeling approaches may exhibit varying strengths in handling different types of questions.
This project compares traditional convolutional neural network (CNN)-based methods and vision-language approaches for radiology visual question answering using the VQA-RAD dataset. Preliminary experiments indicate that CNN-based models are effective for certain closed-ended questions, while vision-language models show greater potential for open-ended questions. This study establishes a foundation for a more comprehensive comparative evaluation in later stages of the project.

